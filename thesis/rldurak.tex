\documentclass[a4paper,titlepage]{article}

\hyphenation{Du-rak}

\usepackage{amsmath, amssymb}

\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}

\usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage[hidelinks]{hyperref}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{booktabs}

\widowpenalty=300
\clubpenalty=300

\DeclareMathOperator{\Expectation}{\mathbb{E}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\Exp}[3]{\Expectation_{#1} \left[ #2 \ \middle| \ #3 \right]}
\newcommand{\Ex}[2]{\Expectation_{#1} \left[ #2 \right]}

\newcommand{\expn}[2]{{#1}\mathrm{e}{#2}}

\title{Reinforcement Learning for Durak \\ \medskip \large{Bachelor's Thesis on Reinforcement Learning}}
%\subtitle{Bachelor's Thesis on Reinforcement Learning}
% TODO remove matriculation number from history https://stackoverflow.com/questions/7194939/git-change-one-line-in-file-for-the-complete-history
\date{\today}

\pagenumbering{roman}

\begin{document}

\maketitle

\setcounter{page}{2}
\thispagestyle{empty}
\noindent
\textbf{Declaration of Authorship} \medskip

\noindent
This thesis and the corresponding code have been written solely by myself, Jan Ebert. All sources used have been cited and referenced. \bigskip

\noindent
\makebox[3cm]{\hrulefill} \hspace{0.2cm} \makebox[4cm]{\hrulefill}

\noindent
\makebox[3cm][l]{\textit{Signature}} \hspace{0.2cm} \makebox[4cm][l]{\textit{Place and date}}

\begin{abstract}
\setcounter{page}{3}
I adapt the successful deep deterministic policy gradient algorithm to the card game durak. The proposed model is supposed to work for most variants of durak. The model successfully learns durak's rules with minimal prior knowledge % TODO or without?
and demonstrates better results than simply playing at random.
\end{abstract}

\setcounter{page}{4}
\thispagestyle{empty}
\tableofcontents

\newpage

\pagenumbering{arabic}

\section{Introduction}

Reinforcement learning -- and machine learning in general -- is one of the most progressive scientific fields in present time. Learning like a human by trial and error is fascinatingly useful and can have large influences in a lot of very different subjects. Because computers are much faster at calculating than a human and can concentrate solely on the problem at hand, not needing time to, for example, do the dishes, they can find correlations that even experts of certain topics have not discovered yet; simply by magnitude of observations.

However, reinforcement learning is also not easy. To try and understand reinforcement learning, I model the Russian card game durak and let an agent learn it. It does that by first playing against players that only execute random actions and then, when it has learned enough to understand the basics of the game, by playing and improving against itself. I want to solve the game without explicitly modeling it by calculations of probabilities. The agent simply plays the game and learns to find regularities and an optimal strategy without further interaction from my side.

Durak is an interesting game for this task for many reasons: First of all, it has many variables. In durak, the amount of cards in the deck, amount of cards in hand or even amount of players can vary. The amount of players in the game varies even while playing: Winners are removed from the game while the others continue to compete. It is a competitive multi-agent game with limited information up until the end when the remaining players have perfect information. In each game state, a selection from a lot of very different actions can be made. Each decision might end up changing the end result of the game.
Because durak is played in real time, it is interesting to see how the agent deals with actions of other players that influence the game's state independently from the agent.
Also, durak might simply be a game of luck. Testing whether skill is involved and how much it can influence the outcome of a game will be tested by the learning agent.

To summarize, I apply what I have not learned to what I \emph{have} learned during my time in university -- durak.

\newpage

\section{Durak}

Durak, like most older card games, can have a lot of variations, starting with the number of cards and players. A standard 52-card deck is used to play. The classic Russian version is played with 36 cards by removing the cards 2 to 5 of every suit. Durak can be played with any amount of cards divisible by four though (as there are four suits). Multiple decks can even be combined, duplicates are not treated differently.

The goal of the game is to get rid of one's cards so as not to be the durak (Russian: Дурак, meaning ``fool''). First, a deck is shuffled and each player is dealt six cards (or, for example, only five if there are not enough cards -- durak is very flexible) and one card is laid on the table for everyone to see. This is the card determining the trump suit for the current game. The deck is placed on the open card as a talon. The revealed card is part of the talon, so the last card drawn is always a trump.
There are two phases in the game: When the talon is not empty, players redraw cards from it and can not yet win. When it has been completely drawn, players can start emptying their hands and ultimately win. However, this is not exactly correct, as in durak, there is no winner, only a loser. I will, however, use the term winner for ``not loser''. \medskip

The player with the lowest trump begins the game (if there are duplicates and several players have the lowest trump, the next lowest trump could be taken instead). They attack with a card of their choice that the next player in clockwise order then has to defend by placing a card of the same suit with a higher value onto it. The defending player can also use a trump to defend any card except another trump with a higher value. The defender's neighbours, the first and second attacker, can place any card whose value is already on the table as another attack (so the attackers can also instantly place two or more cards of the same value on the table). They can do this until there are either six (or, depending on the hand size, another value) attack cards on the table or as many attack cards as the defender has cards remaining in his hand. In classic durak, only five cards can be placed before the first successful defense but I will do without this rule.

If the defense is successful, that means that no attacker can or wants to put another card as an attack (they \emph{check}), all cards on the table are removed and the players draw from the talon in order of attackers with the defender always drawing last. The defender then attacks the next player.
If the defending player cannot defend all cards, they must pick up every card on the table and cannot start attacking the next round. Instead, the player coming after the defender begins after the players drew cards like before. When the defender signals that they cannot defend (this is also checking), the attackers can still place (legal) cards that the defender also has to take. As before, the amount of attack cards cannot exceed the hand size or the amount of cards in the defender's hand.
When a player has no cards left, they are out of the game and cannot become durak. In most versions, the finished player's place is taken by the player coming before them only after an attack has ended but in my version, they immediately move up.
Another rule that is not in classic durak is pushing. Pushing means that, when no card has been defended, the defender can put a card of the same value as the attacking card (or cards) next to it (instead of covering it) and \emph{push} the attack to the next player who then has to defend all cards as usual. The new defender's neighbours are the only attackers, so the original first attacker has no further part in the attack (unless they are the defender's neighbour again).

The last player with cards in hand is the loser and receives the title of ``durak''. They have to shuffle and deal the cards for the next game and are the first to be attacked. When a new player joins the others or someone leaves, it is a new game and the player with the lowest trump begins the game instead as if there was no durak. If no player has a trump in hand, I randomly select a beginner. \medskip

Durak is also notorious for being a game that allows cheating. Players can place anything they want on the table. Even if they are caught, there is no penalty -- they just have to take the illegal card(s) back in hand. While this is great for teaching new players the rules of the game, a learning agent would be distracted by it so I will only focus on ``fair'' durak.

In Wikipedia~\cite{wikidurak}, durak refers to the page ``Games of skill'' in its section ``See also''. With the learning agent, I want to test whether that reference is correct. While there is randomness and luck involved in the outcome of a game, the agent should get a higher win rate (non-loss rate) than players acting at random. \medskip

The version that I will mostly concentrate on in the experiments is a downscaled one played with a deck of 12 cards, 3 cards in hand and only two players.
Pushing and immediate moving up when a player is out is allowed.

\newpage

\section{Reinforcement Learning}

Reinforcement learning is an area of machine learning trying to model human trial-and-error learning mathematically. Sutton and Barto summarize it like this: ``Reinforcement learning is a computational approach to understanding and automating goal-directed learning and decision-making. It is distinguished from other computational approaches by its emphasis on learning by an agent from direct interaction with its environment, without relying on exemplary supervision or complete models of the environment.''~\cite[p.~15]{book} At time $t$, an agent interacts with an environment's state $s_t$ by executing an action $a_t$ that is chosen based on a learned policy $\pi$. A policy can be understood as a mapping from a state to an action that is then taken. After executing the action, the agent receives a reward $r_t$ and observes a new state $s_{t+1}$. Learning is based on maximizing the expected, cumulative future reward, also called the \emph{return}. The return is usually discounted by a factor $0 \leq \gamma \leq 1$ to control the agent's foresight. The greater $\gamma$, the more strongly future rewards are taken into account. Thus, for the return at time $t$, we get
\begin{equation*}
  R_t \doteq r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... = \sum_{k = 0}^{\infty} \gamma^k r_{t+k+1}.
\end{equation*}
These returns can be used to calculate how good taking a certain action $a_t$ and then following policy $\pi$ is in a given state $s_t$:
\begin{equation*}
  q_\pi(s_t, a_t) \doteq \Exp{\pi}{R_t}{s_t, a_t} = \Exp{\pi}{\sum_{k = 0}^{\infty} \gamma^k r_{t+k+1}}{s_t, a_t},
\end{equation*}
which is the \emph{action-value function} of policy $\pi$.
$q_\pi$ can be estimated from experience which is key to reinforcement learning.
Also, $q_\pi$ satisfies a recursive relationship called the \emph{Bellman equation}:
\begin{align*}
  q_\pi(s_t, a_t) &= \Exp{\pi}{R_t}{s_t, a_t} \\
  &= \Exp{\pi}{r_{t+1} + \gamma R_{t + 1}}{s_t, a_t} \\
  &= \Ex{r_{t+1}, s_{t+1}}{r_{t+1} + \gamma \Exp{\pi, a_{t+1} \sim \pi}{R_{t+1}}{s_{t+1}, a_{t+1}}} \\
  &= \Ex{r_{t+1}, s_{t+1}}{r_{t+1} + \gamma q_\pi(s_{t+1}, a_{t+1}},
\end{align*}
where $a_{t+1} \sim \pi$ is the action taken with respect to $\pi$.
$q_\pi$ can only be written without the second expectation if $\pi$ is deterministic, meaning that $\pi$ does not model probabilities of choosing an action but instead maps to an explicit action.

We seek an optimal policy $\pi_*$ that fulfills $q_{\pi_*}(s, a) \doteq \max_\pi q_\pi(s, a)$. If $\pi_*$ is found, the reinforcement learning problem is solved. This optimal policy can only be defined for \emph{finite Markov decision processes}. A reinforcement learning problem is called a Markov decision process if a state can summarize all information that matters for the present and future of the problem. It is a finite Markov decision process if its action and state spaces are finite. However, this is mostly important for the theory of reinforcement learning and since durak cannot be viewed as a finite Markov decision process because of its imperfect information and no prior knowledge of the other players' behaviour (which will influence future game states), it is usually okay to not have perfect information. Meaning that we do not need a perfect representation of the game's state to achieve good results. This can be compared to humans playing a card game: It is often enough to track only the most important cards instead of every single one. There is no need to remember how many beads of sweat were on a player's forehead when they made a questionable decision. \medskip

The environment providing each state can be modeled, meaning that for a given state, every probability for each new state is defined. These probabilities are used to plan actions by considering all possible future states and their chance of occurring. This is in contrast to simply learning by trial and error on a simulated model. It is much easier to simulate a complex problem than it is to perfectly model its environment. That is why I am going to concentrate on \emph{model-free} algorithms that rely on simulations instead of doing the Sisyphean work of calculating every single probability of durak.

A central idea of reinforcement learning is that of temporal-difference learning. Changes in training are based on the comparison, or the \emph{difference}, of the old state to the new state. The method of only calculating the difference of the old state to the directly following state is called TD(0). TD(1) calculates the difference from the very last state received to the first observed state. In between those two are TD($\lambda$) methods with $0 \leq \lambda \leq 1$ which weigh future states depending on $\lambda$, similar to the discount factor $\gamma$. We will focus only on TD(0) which is the basis of the backups considered in this thesis. Backups are changes during learning where new values are calculated from older values. The new value is \emph{backed up} from the old value.

A big problem in reinforcement learning is the balance of exploitation against exploration. Exploitation is simply taking the action with the most return associated to it, so the acquired knowledge is exploited. But it is also important to consider other actions and learn that they might offer an even better return than the action suggested by the current policy. That is exploration: Trying out new actions and observing whether they are useful. To solve this problem, often, an $\epsilon$-greedy strategy (with $0 \leq \epsilon \leq 1$) is applied. The action with the maximum associated return is taken $(1 - \epsilon)$ of the time (the greedy aspect) and a random action is taken with probability $\epsilon$, so $\epsilon$ manages the amount of exploration by the agent. Because of their simplicity and effectiveness, $\epsilon$-greedy strategies are the most used in reinforcement learning. I will also apply an $\epsilon$-greedy strategy wherein $\epsilon$ is decreased during learning. At the beginning, the agent will only explore ($\epsilon = 1$). With ongoing time it will exploit more and more often; $\epsilon$ will never decrease to 0, though, to still guarantee exploration.

Learning can be done on- and off-policy. On-policy learning is simply applying the policy that is also learned while in off-policy learning, one policy is applied and its actions taken but another policy is learned. For example, the learned policy could be completely greedy while the one applied is an $\epsilon$-greedy policy. This method also helps with exploration as the applied policy can generate any data that the learned policy then uses to update its assumptions even though the learned policy would never have received that data if it was also applied. Learning off-policy is useful to stabilize learning and therefore very important for this thesis (more on that in \hyperref[sec:anns]{section~\ref*{sec:anns}}). \medskip

The first parts of this section were notably influenced by the third chapter of the Sutton and Barto book on reinforcement learning~\cite{book}.

\subsection{Artificial Neural Networks}
\label{sec:anns}

For small state and action spaces, every state's action-value function for each action can be stored in a table and retrieved as desired. In real-world applications, however, there are usually a lot of actions possible in a lot of states. State and action spaces can even be continuous with an infinite number of possibilities (only almost infinite in practice since computers cannot represent all real numbers). Storing each value would take absurd amounts of memory and learning each value would take way too much time. So the agent needs to generalize from states it has seen to similar ones. To solve this problem, the action-value function is only approximated using parameters $\theta$ instead of fully solved. So $q(s, a | \theta)$ is the action-value function for state s and action a given the parameters $\theta$. There are a lot of different function approximators in the field of machine learning, most notably artificial neural networks. Neural networks are universal, nonlinear function approximators, meaning that they can approximate any possible function up to a certain error, based on -- as the name implies -- networks of neurons. A neural network consists of layers of neurons that are usually fully connected in a feed-forward manner. Each neuron sums its incoming activations multiplied by individual weights and then applies a nonlinear activation function on that sum.  % TODO visual
It is important that the activations are nonlinear; else, neural networks would just be linear function approximators and lose their unique capabilities.
Neural networks' weights $\theta$ can be trained by methods of gradient descent on an error or loss function. % TODO more on gradient descent?

However, large neural networks were known to not converge for most action-value functions. This changed when the deep Q-network created by DeepMind Technologies (now Google DeepMind) achieved great results on learning a lot of different Atari games purely by pixel data using the same architecture for all games~\cite{nature}. There are two very relevant ideas for stabilization. The first is \emph{experience replay} which means that experience tuples $(a_t, s_t, r_t, s_{t+1})$ are stored in memory and instead of training on the most recent experience, a random batch of experiences is selected. The neural network then trains on that batch to minimize correlations between experiences and improve speed (because most machine learning software libraries are optimized on batch learning). The second measure taken is that instead of directly training the network, a different \emph{target network} is used to train the actual network. This improves the consistence of the targets calculated for training because the model does not change as quickly. The target network is only updated to a copy of the actual network every $C$ steps. Sadly, the deep Q-network is not feasible for the task at hand because the network requires discrete actions without parameters (more on that in \hyperref[sec:challenges]{section~\ref*{sec:challenges}}). Luckily, this problem has been solved by the deep deterministic policy gradient~(DDPG) algorithm~\cite{ddpg}, also developed by Google DeepMind, which can operate over continuous action spaces.

\subsection{Deep Deterministic Policy Gradient}

DDPG is a model-free, off-policy, actor-critic algorithm that was designed to solve tasks on the continuous action domain. \emph{Actor-critic} means that instead of learning an action-value function and choosing an action based on that, meaning that the policy is inferred by the action-value estimates, a parameterized policy is learned that selects actions without being based on the action-value function. The action-value function is still important to learn the policy's parameters, so it also needs to be learned even though it is not required for action selection. Thus, there are two functions to learn: One calculates which action to take in a given game state, the ``actor'', and one approximates the action-value function so as to give the actor feedback on its decision and help it learn. Therefore, that function is called the ``critic''.

DDPG uses two artificial neural networks for its approximations, one each for the actor and critic network with the corresponding weight vectors $\theta_A$ and $\theta_C$ and functions $\pi(s_t | \theta_A)$ and $q(s_t, a_t | \theta_C)$. Our goal is to learn a policy that maximizes the expected return from the start distribution $J = \Ex{r_i, s_i; a_i \sim \pi}{R_1}$. The critic learns using the Bellman equation while the actor optimizes the gradient of its policy's performance, the \emph{policy gradient}, defined like so:
\begin{align*}
  \nabla_{\theta_A} J &\approx \Exp{s_t \sim \rho_\beta}{\nabla_{\theta_A} q(s_t, a | \theta_C)}{s_t, a = \pi(s_t | \theta_A)} \\
  &= \Exp{s_t \sim \rho_\beta}{\nabla_a q(s_t, a | \theta_C)}{s_t, a = \pi(s_t) \nabla_{\theta_A} \pi(s_t | \theta_A)},
\end{align*}
where $J$ is the start distribution with respect to the actor's parameters and $\rho_\beta$ is the discounted state visitation distribution for policy $\beta$ from which is learned (as DDPG is off-policy).

To stabilize learning, the two main methods used by the deep Q-network are also applied: Experience replay and having a second target network. The target network, however, has been modified to slowly track the actual network's learned values instead of copying it every $C$ steps.

\newpage

\section{Challenges}

\label{sec:challenges}
In durak, there are many different states of the game that can occur. Therefore, the problem cannot be solved using tabular methods. Instead, two artificial neural networks are trained: One for the actor and one for the critic as used by the DDPG algorithm.
Another problem is that the action space is very large. There are only five different types of actions -- attacking, defending, pushing, checking and simply waiting. However, the actions for attacking, defending and pushing have to specify in which way to interact with the game. In the case of attacking and pushing, which card to put on the table needs to be specified. For defending, which card to defend is also necessary information. These cards are the action's parameters, that is why these actions are called \emph{parameterized actions}. The actions themselves are discrete but they accept parameters that specify what the action does. The parameters are also discrete and in a given state, there is only a limited number of possible parameters. For the neural network that does not know the game's rules, however, any card could be put anywhere. This problem can be solved simply by giving the agent a big negative reward when it chooses an action that is not allowed. While this approach works, it takes a lot of time that could be used to learn useful strategies and might also influence other decisions because of the network's generalization capabilities. Given enough time, the agent should learn to not execute illegal actions but still, the problem of not selecting perfectly legal actions may also occur.

Furthermore, it is not easy to find a general solution for the state vector (or feature vector; I will use these two terms interchangeably) that defines the game's state at a given point in time for arbitrary parameters of durak (amount of cards in deck, hand size, ...). The size of the vector should be the same for all durak parameters so that it can be used by a neural network already trained on other parameters.
We could index where a card is located from the agent. This would not consume a lot of memory and the feature vector would have the same size for any amount of players. If the deck has duplicates, the problem can be solved by creating two features per card.
Simply creating a binary feature for each card for each player that determines whether the player holds that card would, contrary to the previous idea, result in a state vector with variable size depending on how many players there are. Because of its large size, this type of feature vector consumes a lot of memory and requires a lot of additional computations for each training step if there are a lot of players and cards.
This idea can also be generalized for any amount of players thereby solving its memory issues. One could, for example, only store the card information for the two neighbouring players since they are the most important to the agent. Although information is lost when doing this, the state definition can then also be used for any amount of players and cards if the deck has no duplicates. To also have a general state vector for duplicates, we could store how many of each card the two players have.

The end goal is to find an agent that can play any durak game well. Using a general state vector, one can test whether the agent is able to generalize on other variants of durak which would be very interesting. 
Because the amount of cards in the deck also changes, the vector's size can be buffered to contain the maximum allowed number of cards (52 for a standard deck without duplicates). Only the cards in the game are tracked while the other ones are seen as out of the game. The generalization capabilities of the neural networks should make this approach possible.

Since the game is played in real time and the actions of the other players influence the game state, it is very hard for the agent to understand state changes. For example, assume that the agent defends and checks. The other players now attack with their cards until the maximum allowed number of cards is placed so the attack ends. Now the agent only sees the state of when it checked and the state when the attack ended. Thus, the agent learns that checking changed much more in the game than it actually did because a lot has happened in between that the agent had no influence on. Also, it is hard to assign which action was actually a good one. A game of durak can be won because of a lot of different decisions. Sometimes it is good to keep a low-value card because it can be used for pushing while in other cases it will be stuck in hand until the very end when it should not be used for attacking because it can easily be defended. It is really hard to put a finger on when an intelligent decision occured because there are many decisions and the actual, ``real'' reward (in the sense that it is not modeled but part of the game) is only received at the very end when it is clear that the agent has won or lost. To heuristically solve this, reward can be given based on the difference in average card value or amount of trumps in hand. If either of these increases after an attack, it is usually positive for the player because the stronger cards can be used later to defend more easily or to place an attack that the other player cannot defend. This problem of credit assignment cannot easily be solved though. A lot of planning has to go into why a decision is good or why it is not and reward given during the game has to be based on that.

\newpage

\section{Implementation}

For where to find the code, see \hyperref[sec:appendix]{section~\ref*{sec:appendix}}.

The simulator for games of durak was implemented in Python~\cite{python} for versions 2.7 and 3.5. For the neural networks, the machine learning library Keras~\cite{keras} was used with {TensorFlow}~\cite{tensorflow}  as its backend. I have used Python~3.5 even though TensorFlow for Python~3.6 has been implemented because it is currently labeled unstable. I will refer to players that are not learning as \emph{bots} or \emph{random bots} because they are acting at random. The players that do learn are \emph{agents} like before.

To keep the simulated games as authentic as possible, I use a thread for each player for the real time aspect. The random bots' behaviour is based on probabilities. With a certain probability, the bots wait and with another, they check. These probabilities are calculated each game based on a normal distribution with a given mean and variance and clipped to have values that make sense. For example, a player cannot always wait (because they have to check at some point) but they \emph{can} never check (therefore always playing all their cards). The game is fully customizable supporting different deck and hand sizes (even duplicates), a variable amount of players, and an optionally fixed trump suit. To make learning easier, I always assign hearts as the trump suit. The suit itself does not matter but when loading an agent, it should be the same as it was trained on. Also, random bots can be turned into agents so that only agents play against each other. When the agent outclasses the random bots, this is the next step for it to learn even further.

Cards have a numerical value ranging from 0 to 12 and suit ranging from 0 to 4. For generalization purposes, these values are fixed, even if cards have been removed. (An ace will always have value 12, no matter how many cards have been removed.) \medskip

Three different types of feature vectors that all support size buffering have been implemented according to the explanations in \hyperref[sec:challenges]{section~\ref*{sec:challenges}}.
As an example to explain the differences between the types, assume that the agent is the player at index 1 holding the ten of spades (T$\spadesuit$), the player at index 0 has a ten of hearts (T$\heartsuit$) and another player is at index 3 holding the ten of diamonds (T$\diamondsuit$). A jack of clubs (J$\clubsuit$) is on the field and a queen of clubs (Q$\clubsuit$) had previously been successfully defended and is out of the game. In the deck, yet to be seen, is a king of clubs (K$\clubsuit$) that no player has any information about. At the bottom of the deck as the revealed trump lies the ace of clubs (A$\clubsuit$). The deck contained 52 cards at the beginning and there are four players in the game, so indices range from 0 to 3. I will also list the size of the vectors for this example, adding 3 extra features that I will discuss later.
\begin{enumerate} % TODO visualization
  \item Indices from the agent to locate cards; $52 + 3 = 55$~features. The values in our example would be (T$\spadesuit$:~0), (T$\heartsuit$:~3) and (T$\diamondsuit$:~2). Index values wrap around so they are always positive. Cards on the field have value~$-1$, cards out of the game have~$-2$, cards without a known location have~$-3$ and the bottom trump has~$-4$, so the remaining cards' feature values are (J$\clubsuit$:~$-1$), (Q$\clubsuit$:~$-2$), (K$\clubsuit$:~$-3$) and (A$\clubsuit$:~$-4$).
  \item Binary features for each player for each card; $(4 + 2) \cdot 52  + 1 + 3 = 316$~features. Because the cards are treated separately for each player, we assign an index to each player at which the cards are stored and use index $F$ to denote where the locations of cards on the field start and $G$ to denote the ones for cards out of the game. The locations of the players in the feature vector are ordered like in the first example by indices from the agent. The values in the example are (T$\spadesuit$,~0:~1), (T$\spadesuit$,~$\{1, 2, 3, F, G\}$:~0); (T$\heartsuit$,~3:~1), (T$\heartsuit$,~$\{0, 1, 2, F, G\}$:~0); (T$\diamondsuit$,~2:~1), (T$\diamondsuit$,~$\{0, 1, 3, F, G\}$:~0); (J$\clubsuit$,~$F$:~1), (J$\clubsuit$,~$\{0, 1, 2, 3, G\}$:~0); (Q$\clubsuit$,~$G$:~1), (Q$\clubsuit$,~$\{0, 1, 2, 3, F\}$:~0).
  The king and ace are simply zero for all indices because they have no known location yet: (K$\clubsuit$,~$\{0, 1, 2, 3, F, G\}$:~0) and (A$\clubsuit$,~$\{0, 1, 2, 3, F, G\}$:~0). To still use the information of the bottom trump, I add an extra feature that stores the numerical value of it, so that feature's value is 12.
  \item Amount of each card in each neighbour's hand; $(2 + 2) \cdot 52 + 1 + 3 = 212$~features. This is the previous feature vector but reduced to be used for any amount of players. Since this is a smaller version of the features of type 2, the values in the example are the same except for the ones for the player with index 3 who is not observed (because they are not a neighbour of the agent). Those values simply do not exist.
\end{enumerate}
Now there are three extra features in each vector that give crucial information to the game: One for the numerical suit the next player could not defend, one for whether the defending player checks and one for the amount of cards left in the deck.

Rewards are given after each action except when the turn ends. The rewards given during an attack are 0 as not much can be said about what will happen. At the end of the turn, the agent receives a reward based on three different factors: The difference in the average value of the cards in hand except trump cards, the difference in the average value of trump cards in hand and the difference in the amount of trump cards in hand. Each of these factors is normalized and weighted to give control over how much value is assigned to each factor.
Let $v$ be the average card value in hand without trumps, $t$ the average trump value in hand and $c$ the amount of trump cards in hand. A subscripted $b$ denotes the value at the beginning of the turn while an $a$ is for after the turn has ended. The weights for each value are a $w$ with the value's symbol subscripted. As a formula for the end of turn reward, we get
\begin{equation*}
  R_{end\ of\ turn} = w_v \cdot (v_a - v_b) + w_t \cdot (t_a - t_b) + w_c \cdot (c_a - c_b).
\end{equation*}
This value is also normalized to be between $-1$ and 1 (by normalizing the input $w$ to sum to 1). Rewards for winning and losing are awarded when the game ends. When the agent waits, it receives a small negative reward so that it will not indefinitely wait. Also, the agent has to learn which actions are legal and which are not. That is why, when the agent tries to execute an illegal action, I store the experience with an unchanged game state and give a large negative reward. This can also be deactivated. The agent then waits and tries again. With probability $\epsilon$, the agent chooses a random legal action. $\epsilon$ is linearly annealed from a starting value to a minimum value in the first $C$ episodes.

To have some sophisticated behaviour, the random bots wait until all current attacks are defended or the defender checks until they place another one. This is usually the best strategy unless a player wants to get rid of their cards before the maximum amount of cards is placed. This is also implemented for agents but can be toggled to give the agent more freedom in decision making. Assuming the bots' behaviour is not the optimal strategy -- which is highly likely -- the agent should achieve better results than the random bots.

Excluding the amount of parameters for the game itself, over 30 different parameters can be tuned that each affect the agent's learning.
Because the number of parameters is this large, they simply cannot all be tuned. I have relied on the parameters in the DDPG paper~\cite[p.~11]{ddpg} and try fitting them to the problem at hand during the different experiments.
As the optimizer for the neural networks, I have mostly used Adam~\cite{adam}. This decision was also influenced by the DDPG paper. However, RMSProp~\cite{rmsprop} can also be used and was also tested in the experiments.
To make comparisons of experiments easier, I also implemented a script that plots the average win rates over a given amount of games which helps to see the change of results. \medskip

The code for the actor and critic networks and subsequently their learning was heavily based on Ben Lau's~\cite{torcs} who also used Keras with TensorFlow to train an agent for the racing game TORCS (recreating the DDPG paper's results in Python).

\newpage

\section{Experiments}

The experiments in this section were mostly conducted with different versions of the code because improvements were continuously added. In the earlier experiments, some data was lost due to access issues but since those were more primitive approaches, it should not be much of an issue.

Recall that in this section, I will focus on a reduced version of durak with only 12 cards in the deck, 3 cards in hand and two players. The most prevalent reason for this is time: Not only do the games finish earlier but also the feature vector is smaller leading to fewer computations per training step.
Also, because the game is simpler, it is easier for the agent to learn. When good results for the downscaled version are achieved, the parameters found can be applied to the full game.

The low win rate of the agent at the beginning of the experiments can be explained because the agent and the bot do not execute the same random behaviour. While the bot has a fixed probability for checking each game, the agent will check with the same probability as any other action.



\newpage

\section{Appendix}
\label{sec:appendix}

The code is available online on GitHub at \url{github.com/janEbert/rldurak}. \bigskip

\begin{table}[h]
\centering
  \begin{tabular}{lrl}
    \toprule
    Parameter & Value & Description \\
    \midrule
    episodes & 10000 \\
    feature type & 2 \\
    $\epsilon$ start & 1 & linearly anneal min $\epsilon$ from this\\
    min $\epsilon$ & 0.1 & $\epsilon$ does not decrease further than this \\
    $\epsilon$ episodes & 6000 & linearly anneal in this many episodes \\
    optimizer & Adam \\
    $\alpha$ actor & 0.001 & learning rate \\
    $\alpha$ critic & 0.01 \\
    $\varepsilon$ actor & $\expn{1}{-8}$ & $\varepsilon$ for numerical stability \\
    $\varepsilon$ critic & $\expn{1}{-8}$ \\
    $\tau$ actor & 0.01 & target model update factor \\
    $\tau$ critic & 0.01 \\
    $n_1$ actor & 100 & neurons in the first hidden layer \\
    $n_1$ critic & 100 \\
    $n_2$ actor & 50 & neurons in the second hidden layer \\
    $n_2$ critic & 50 \\
    $\gamma$ & 0.99 & discount factor \\
    max experience count & 500 \\
    batch size & 32 \\
    win reward & 12 \\
    loss reward & ${-12}$ \\
    wait reward & $-0.05$ \\
    illegal action reward & $-100$ \\
    $w_v$ & 1 & weight for difference in mean card value \\
    $w_t$ & 2 & weight for difference in mean trump value \\
    $w_c$ & 2 & weight for difference in trump amount \\
    $\psi_\mu$ & 0.95 & mean value for how often bots wait \\
    $\psi_\sigma$ & 0.1 & standard deviation for how often bots wait \\
    $\chi_\mu$ & 0.08 & mean value for how often bots check \\
    $\chi_\sigma$ & 0.1 & standard deviation for bots checking \\
    wait until defended & true & agent attacking behaviour \\
    \bottomrule
  \end{tabular}
  \caption{Default parameters for the experiments}
  \label{table:parameters}
\end{table}

\newpage

\begin{thebibliography}{99}

\bibitem{wikidurak}
  Wikipedia contributors. ``Durak.'' \emph{Wikipedia, The Free Encyclopedia}. Wikipedia, The Free Encyclopedia, 31 Aug. 2017. Web.
  7 Sep. 2017., \\
  \url{en.wikipedia.org/w/index.php?title=Durak&oldid=798247605}

\bibitem{book}
  Sutton, Richard S., and Andrew G. Barto. ``Reinforcement Learning: An Introduction.'' \emph{Reinforcement Learning: An Introduction}. Rich Sutton's Home Page, 19 Jun. 2017. Web.
  20 Jun. 2017.,
  \url{incompleteideas.net/sutton/book/bookdraft2017june19.pdf}

\bibitem{nature}
  Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, et al. ``Human-level control through deep reinforcement learning.'' \emph{Nature}. Nature, 518: 529-533, 26 Feb. 2015.

\bibitem{ddpg}
  Lillicrap, Timothy P., Jonathan J. Hunt, et al. ``Continuous control with deep reinforcement learning.'' \emph{arXiv.org}. arXiv:1509.02971v5, 29 Feb. 2016. Web.
  10 Sep. 2017.,
  \url{arxiv.org/pdf/1509.02971v5}

\bibitem{python}
  van Rossum, Guido, et al. ``Python.'' \emph{Python}. Python Software Foundation, 20 Feb. 1991. Web.
  2017.,
  \url{python.org}

\bibitem{keras}
  Chollet, Fran\c{c}ois, et al. ``Keras.'' \emph{Keras}. GitHub, 27 Mar. 2015. Web.
  2017.,
  \url{github.com/fchollet/keras}

\bibitem{tensorflow}
  Abadi, Martín, et al. ``TensorFlow: Large-scale machine learning on heterogenous systems.'' \emph{TensorFlow}. Google Brain Team, 9 Nov. 2015. Web.
  2017.,
  \url{tensorflow.org}

\bibitem{adam}
  Kingma, Diederik P., and Jimmy Lei Ba. ``Adam: A Method for Stochastic Optimization.'' \emph{arXiv.org}. arXiv:1412.6980v9, 30 Jan. 2017. Web.
  19 Sep. 2017.,
  \url{arxiv.org/abs/1412.6980v9}

\bibitem{rmsprop}
  Hinton, Geoffrey, Nitish Srivastava, Kevin Swersky. ``Neural Networks for Machine Learning.'' \emph{Department of Computer Science, University of Toronto}. CSC321 Winter 2014 -- Introduction to Neural Networks and Machine Learning, Lecture 6, 11 Feb. 2015, Web. 23 Sep. 2017.,
  \url{www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}

\bibitem{torcs}
  Lau, Ben. ``Using Keras and Deep Deterministic Policy Gradient to play TORCS.'' \emph{GitHub Pages}. GitHub, 11 Oct. 2016. Web.
  8 Sep. 2017.,
  \url{yanpanlau.github.io/2016/10/11/Torcs-Keras.html}

\end{thebibliography}

\end{document}