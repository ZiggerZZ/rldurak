\documentclass[a4paper,titlepage]{article}

\hyphenation{Du-rak}

\usepackage{amsmath, amssymb}

\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}

\usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage[hidelinks]{hyperref}
\usepackage[nottoc,numbib]{tocbibind}
%\usepackage{booktabs}

\widowpenalty=300
\clubpenalty=300

\DeclareMathOperator{\Expectation}{\mathbb{E}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\Exp}[3]{\Expectation_{#1} \left[ #2 \ \middle| \ #3 \right]}
\newcommand{\Ex}[2]{\Expectation_{#1} \left[ #2 \right]}

\title{Reinforcement Learning for Durak \\ \medskip \large{Bachelor's Thesis on Reinforcement Learning}}
%\subtitle{Bachelor's Thesis on Reinforcement Learning}
\author{Jan Ebert \\  \\ \small{Cognitive Informatics} \\ \small{Faculty of Technology}}
\date{\today}

\pagenumbering{roman}

\begin{document}

\maketitle

\setcounter{page}{2}
\thispagestyle{empty}
\noindent
This thesis has been written solely by myself, Jan Ebert. All sources have been referenced.

\begin{abstract}
\setcounter{page}{3}
I adapt the successful deep deterministic policy gradient algorithm to the card game durak. The proposed model is supposed to work for most variants of durak. The model successfully learns durak's rules and demonstrates better results than simply playing at random.
\end{abstract}

\setcounter{page}{4}
\thispagestyle{empty}
\tableofcontents

\newpage

\pagenumbering{arabic}

\section{Introduction}

Reinforcement learning -- and machine learning in general -- is one of the most progressive scientific fields in present time. Learning like a human by trial and error is fascinatingly useful and can have big influences on a lot of very different subjects. Because computers are much faster at calculating than a human and can concentrate solely on the problem at hand, not needing time to, for example, do the dishes, they can find correlations that even experts of certain topics have not discovered yet; simply by magnitude of observations.

However, reinforcement learning is also not easy. To try and understand reinforcement learning, I model the Russian card game durak and let an agent learn it. It does that by first playing against players that only execute random actions and then, when it has learned enough to understand the basics of the game, by playing and improving against itself. I want to solve the game without explicitly modeling it by calculations of probabilities. The agent simply plays the game and learns to find regularities and an optimal strategy without further interaction from my side.

Durak is an interesting game for this task for many reasons: First of all, it has many variables. In durak, the amount of cards in the deck, amount of cards in hand or even amount of players can vary. The amount of players in the game varies even while playing: Winners are removed from the game while the others continue to compete. It is a competitive multi-agent game with limited information up until the end when the remaining players have perfect information. In each game state, there are a lot of very different actions. Each decision might end up changing the end result of the game.

In short, I apply what I have not learned to what I \emph{have} learned during my time in university -- durak.

\newpage

\section{Durak}

Durak, like most older card games, can have a lot of variations, starting with the number of cards and players. A standard 52-card deck is used to play. The classic Russian version is played with 36 cards by removing the cards 2 to 5 of every suit. Durak can be played with any amount of cards divisible by four though (because there are four suits). Multiple decks can even be combined, duplicates are not treated differently.

The goal of the game is to get rid of one's cards so as not to be the durak (Russian: Дурáк, meaning ``fool''). First, a deck is shuffled and each player is dealt six cards (or, for example, only five if there are not enough cards -- durak is very flexible) and one card is laid on the table for everyone to see. This is the card determining the trump suit for the current game. The deck is placed on the open card as a talon. The revealed card is part of the talon, so the last card drawn is always a trump.
There are two phases in the game: When the talon is not empty, players redraw cards from it and can not yet win. When it has been completely drawn, players can start emptying their hands and ultimately win. However, this is not exactly correct, as in durak, there is no winner, only a loser. I will, however, use the term winner for ``not loser''. \medskip

The player with the lowest trump begins the game (if there are duplicates and several players have the lowest trump, the next lowest trump could be taken instead). They attack with a card of their choice that the next player in clockwise order then has to defend by placing a card of the same suit with a higher value onto it. The defending player can also use a trump to defend any card except another trump with a higher value. The defender's neighbours, the first and second attacker, can place any card whose value is already on the table as another attack (so the attackers can also instantly place two or more cards of the same value on the table). They can do this until there are either six (or, depending on the hand size, another value) cards on the table or as many cards as the defender has remaining in his hand. In classic durak, only five cards can be placed before the first successful defense but I will do without this rule.

If the defense is successful, that means that no attacker can or wants to put another card as an attack, all cards on the table are removed and the players draw from the talon in order of attackers with the defender always drawing last. The defender then attacks the next player.
If the defending player cannot defend all cards, they must pick up every card on the table and cannot start attacking the next round. Instead, the second attacker begins after the players drew cards like before.
When a player has no cards left, they are out of the game and cannot become durak. In most versions, the player coming before them only takes the finished player's place after an attack has ended but in my version, they immediately move up.
Another rule that is not in classic durak is pushing. Pushing means that, when no card has been defended, the defender can put a card of the same value as the attacking card (or cards) next to it (instead of covering it) and \emph{push} the attack to the next player.

The last player with cards in hand is the loser and receives the title of ``durak''. They have to shuffle and deal the cards for the next game and are the first to be attacked. When a new player joins the others or someone leaves, it is a new game and the player with the lowest trump begins the game instead as if there was no durak. If no player has a trump in hand, I randomly select a beginner. \medskip

Durak is also notorious for being a game that allows cheating. Players can place anything they want on the table. Even if anybody notices their cheating, there is no penalty -- they just have to take the card back in hand. While this is great for teaching new players the rules of the game, a learning agent would be distracted by it so I will only focus on ``fair'' durak.

In Wikipedia~\cite{wikidurak}, durak refers to the page ``Games of skill'' in its section ``See also''. With the learning agent, I want to test whether that reference is correct. While there is randomness and luck in the game, the agent should get a higher win rate (non-loss rate) than a casual player. \medskip

The version that I will mostly concentrate on is played with a deck of 52 cards and 6 cards in hand. Pushing and immediate moving up when a player is out is allowed.

\newpage

\section{Reinforcement Learning}

Reinforcement learning is an area of machine learning trying to model human trial-and-error learning mathematically. Sutton and Barto summarize it like this: ``Reinforcement learning is a computational approach to understanding and automating goal-directed learning and decision-making. It is distinguished from other computational approaches by its emphasis on learning by an agent from direct interaction with its environment, without relying on exemplary supervision or complete models of the environment.''~\cite[p.~15]{book} At time $t$, an agent interacts with an environment's state $s_t$ by executing an action $a_t$ that is chosen based on a learned policy $\pi$. A policy can be understood as a mapping from a state to an action that is then taken. The agent then receives a reward $r_t$ and observes a new state $s_{t+1}$. Learning is based on maximizing the expected, cumulative future reward, also called the \emph{return}. The return is usually discounted by a factor $0 \leq \gamma \leq 1$ to control the agent's foresight. The greater $\gamma$, the more strongly future rewards are taken into account. Thus, we get
\begin{equation*}
  R_t \doteq r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... = \sum_{k = 0}^{\infty} \gamma^k r_{t+k+1}.
\end{equation*}
These returns can be used to estimate how good taking a certain action and then following policy $\pi$ is in a given state:
\begin{equation*}
  q_\pi(s_t, a_t) \doteq \Exp{\pi}{R_t}{s_t, a_t} = \Exp{\pi}{\sum_{k = 0}^{\infty} \gamma^k r_{t+k+1}}{s_t, a_t},
\end{equation*}
which is the \emph{action-value function} of the policy $\pi$.
$q_\pi$ can be estimated from experience and that is one of the key things reinforcement learning accomplishes.
Also, $q_\pi$ satisfies a recursive relationship, called the \emph{Bellman equation}:
\begin{align*}
  q_\pi(s_t, a_t) &= \Exp{\pi}{R_t}{s_t, a_t} \\
  &= \Exp{\pi}{r_{t+1} + \gamma R_{t + 1}}{s_t, a_t} \\
  &= \Ex{r_{t+1}, s_{t+1}}{r_{t+1} + \gamma \Exp{\pi, a_{t+1}\sim\pi}{R_{t+1}}{s_{t+1}, a_{t+1}}} \\
  &= \Ex{r_{t+1}, s_{t+1}}{r_{t+1} + \gamma q_\pi(s_{t+1}, a_{t+1}},
\end{align*}
which can only be written this way if $\pi$ is deterministic, meaning that $\pi$ does not model probabilities of choosing an action but instead is an explicit action.

We seek an optimal policy $\pi_*$ that fulfills $q_{\pi_*}(s, a) \doteq \max_\pi q_\pi(s, a)$. If $\pi_*$ is found, the reinforcement learning problem is solved. This optimal policy can only be defined for \emph{finite Markov-Decision-Processes}. A reinforcement learning problem is called a Markov-Decision-Process if a state can summarize all information that matters for the present and future of the problem. It is a finite Markov-Decision-Process if its action and state spaces are finite. However, this is mostly important for the theory of reinforcement learning and while durak can be viewed as a finite Markov-Decision-Process, it is usually okay to not have perfect information. Meaning that we do not need a perfect representation of the game's state to achieve good results. This can be compared to humans playing a card game. It is often enough to track only the most important cards instead of every single one.

A central idea of reinforcement learning is that of temporal-difference learning. Changes in training are based on the comparison, or the \emph{difference}, of the old state to the new state. The method of only calculating the difference of the old state to the directly following state is called TD(0). TD(1) calculates the difference from the very last state received to the first observed state. In between these two are TD($\lambda$) methods with $0 \leq \lambda \leq 1$ which weigh future states depending on $\lambda$, similar to the discount factor $\gamma$. We will focus only on TD(0) which is the basis of the backups considered in this thesis. Backups are changes during learning based on older values. The new value is \emph{backed up} from the old value.

A big problem in reinforcement learning is the balance of exploitation against exploration. Exploitation is simply taking the action with the most return associated to it, so the knowledge is exploited. But it is also important to consider other actions and learn that they might offer an even better return than the action suggested by the current policy. That is exploration: Trying out new actions and observing whether they are useful. To solve this problem, often, an $\epsilon$-greedy strategy is applied. The action with the maximum associated return is taken $(1 - \epsilon)$ of the time (the greedy aspect) and a random action is taken with probability $\epsilon$, so $\epsilon$ manages the amount of exploration by the agent. Because of their simplicity and effectiveness, $\epsilon$-greedy strategies are the most used in reinforcement learning. I will also apply an $\epsilon$-greedy strategy.

Learning can be done on- and off-policy. On-policy learning is simply applying the policy that is also learned while in off-policy learning, one policy is applied and its actions taken but another policy is learned. For example, the learned policy could be completely greedy while the one applied is an $\epsilon$-greedy policy. This method also helps with exploration as the applied policy can generate any data that the learned policy then uses to update its assumptions. Learning off-policy is useful to stabilize learning and therefore very important for this thesis (more on that in \hyperref[sec:anns]{section~\ref*{sec:anns}}). \medskip

The first parts of this section were notably influenced by the third chapter of the Sutton and Barto book on reinforcement learning~\cite{book}.

\newpage

\section{Artificial Neural Networks}
\label{sec:anns}

For small state and action spaces, every state's action-value function for each action can be stored in a table and retrieved as desired. In real-world applications, however, there are usually a lot of actions possible in a lot of states. State and action spaces can even be continuous with an infinite number of possibilities (only almost infinite in practice since computers cannot represent all real numbers). Storing each value would take absurd amounts of memory and learning each value would take too much of time. So the agent needs to generalize from states it has seen to similar ones. To solve this problem, the action-value function is only approximated instead of fully solved. There are a lot of different function approximators in the field of machine learning, most notably artificial neural networks. Neural networks are universal, nonlinear function approximators, meaning that they can approximate any possible function up to a certain error, based on -- as the name implies -- networks of neurons. A neural network consists of layers of neurons that are usually fully connected in a feed-forward manner. Each neuron sums its incoming activations multiplied by individual weights and then applies a nonlinear activation function on that sum.  % TODO visual
It is important that the activations are nonlinear, else, neural networks would just be linear function approximators and lose their unique capabilities.
Neural networks can be trained by methods of \emph{gradient descent} on an error or loss function.

However, large neural networks were known to not converge for most action-value functions. This changed when the deep Q-network created by DeepMind  Technologies (now Google DeepMind) achieved great results on learning a lot of different Atari games purely by pixel data based on the same architecture~\cite{nature}. There are two very relevant ideas for stabilization. The first is experience replay which means that experience tuples $(a_t, s_t, r_t, s_{t+1})$ are stored in memory and instead of training on the most recent experience, a random batch of experiences is selected. The neural network then trains on that batch to minimize correlations between experiences and improve speed (because most libraries are optimized on batch learning). The second measure taken is that instead of directly training the network, a different target network is used to train the actual network. This improves the consistence of the targets for training because the model does not change as quickly. The target network is only updated to a copy of the actual network every $C$ steps. Sadly, the deep Q-network is not feasible for the task at hand because the network requires discrete actions without parameters (more on that in \hyperref[sec:challenges]{section~\ref*{sec:challenges}}). Luckily, this problem has been solved by the deep deterministic policy gradient~(DDPG) algorithm~\cite{ddpg}, also developed by Google DeepMind, which can operate over continuous action spaces.

\subsection{Deep Deterministic Policy Gradient}

The DDPG algorithm works by learning off-policy, meaning 

\newpage

\section{Challenges}

\label{sec:challenges}
In durak, there are many different states of the game that can occur. Therefore, the problem cannot be solved using tabular methods. Instead, two artificial neural networks are trained. One calculates which action to take in a given game state, the ``actor'', and one approximates the action-value function so as to give the actor feedback on its decision and help it learn. Therefore, that network is called the ``critic''.
Another problem is that the action space is very big. There are only five different types of actions -- attacking, defending, pushing, checking (telling the others that you do not want to attack or defend anymore) and simply waiting. However, attacking, defending and pushing have to specify in which way to interact with the game. In the case of attacking and pushing, which card to put on the table needs to be specified. For defending, which card to defend is also necessary information. Those cards are the action's parameters, that is why these actions are called parametrized actions. The actions themselves are discrete but they accept parameters that specify what the action does. The parameters are also discrete and in a given state, there is only a limited number of possible parameters. For the neural network that does not know the game's rules, however, any card could be put anywhere. This problem can be solved simply by giving the agent a big negative reward when it chooses an action that is not allowed. While this approach works, it might also influence other decisions because of the network's generalization capabilities. Given enough time, it should learn to not execute only illegal actions but still, the problem might occur.

Also, it is not easy find a general solution that defines the game's state at a given point in time. Because the amount of players can vary, one cannot simply create a binary feature for each card for each player that determines whether the player holds that card. The size of the state vector would change depending on how many players there are. Indexing where a card is located from the agent is sadly not a solution because the neural net cannot approximate a function from that information.
To find a general state vector for any amount of players, one could, for example, only store the card information for the two neighbouring players since they are of most importance to the agent. Information is lost when doing this but the state definition can be used for any amount of players and cards if the deck has no duplicates. To also have a general state vector for duplicates, the problem can be solved by storing how many of each card the players have.

% TODO real time, multiple agents! credit assignment problem!

\newpage

\section{Architecture}

\newpage

\begin{thebibliography}{9}

\bibitem{wikidurak}
  Wikipedia contributors. ``Durak.'' \emph{Wikipedia, The Free Encyclopedia}. Wikipedia, The Free Encyclopedia, 31 Aug. 2017. Web.
  7 Sep. 2017., \\
  \url{en.wikipedia.org/w/index.php?title=Durak&oldid=798247605}

\bibitem{book}
  Sutton, Richard S., and Andrew G. Barto. ``Reinforcement Learning: An Introduction.'' \emph{Reinforcement Learning: An Introduction}. Rich Sutton's Home Page, 19 Jun. 2017. Web.
  20 Jun. 2017., \\
  \url{incompleteideas.net/sutton/book/bookdraft2017june19.pdf}

\bibitem{nature}
  Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, et al. ``Human-level control through deep reinforcement learning.'' \emph{Nature}. Nature, 518: 529-533, 26 Feb. 2015.

\bibitem{ddpg}
  Lillicrap, Timothy P., Jonathan J. Hunt, et al. ``Continuous control with deep reinforcement learning.'' \emph{arXiv.org}. arXiv:1509.02971v5, 29 Feb. 2016. Web.
  10 Sep. 2017., \url{arxiv.org/pdf/1509.02971v5}

\bibitem{torcs}
  Lau, Ben. ``Using Keras and Deep Deterministic Policy Gradient to play TORCS.'' \emph{GitHub Pages}. GitHub, 11 Oct. 2016. Web.
  8 Sep. 2017., \\
  \url{yanpanlau.github.io/2016/10/11/Torcs-Keras.html}

\end{thebibliography}

\end{document}